{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-15T08:38:44.215747Z",
     "iopub.status.busy": "2026-01-15T08:38:44.215470Z",
     "iopub.status.idle": "2026-01-15T08:38:44.221826Z",
     "shell.execute_reply": "2026-01-15T08:38:44.220858Z",
     "shell.execute_reply.started": "2026-01-15T08:38:44.215729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !wget -q https://storage.googleapis.com/code-buggy/megadiff_single_function.parquet -O megadiff_single_function.parquet\n",
    "# !wget -q https://github.com/GumTreeDiff/gumtree/releases/download/v4.0.0-beta4/gumtree-4.0.0-beta4.zip -O gumtree-4.0.0-beta4.zip\n",
    "# !unzip -q gumtree-4.0.0-beta4.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:38:44.228813Z",
     "iopub.status.busy": "2026-01-15T08:38:44.228575Z",
     "iopub.status.idle": "2026-01-15T08:38:44.392236Z",
     "shell.execute_reply": "2026-01-15T08:38:44.391218Z",
     "shell.execute_reply.started": "2026-01-15T08:38:44.228790Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.17\" 2025-10-21\n",
      "OpenJDK Runtime Environment (build 17.0.17+10-Ubuntu-122.04)\n",
      "OpenJDK 64-Bit Server VM (build 17.0.17+10-Ubuntu-122.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "# !apt update -qq && apt install -q openjdk-17-jdk -y -qq\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:38:44.395798Z",
     "iopub.status.busy": "2026-01-15T08:38:44.395458Z",
     "iopub.status.idle": "2026-01-15T08:38:44.400266Z",
     "shell.execute_reply": "2026-01-15T08:38:44.399295Z",
     "shell.execute_reply.started": "2026-01-15T08:38:44.395764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %pip install -q javalang\n",
    "# %pip install -q protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AST Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:38:44.401909Z",
     "iopub.status.busy": "2026-01-15T08:38:44.401682Z",
     "iopub.status.idle": "2026-01-15T08:38:44.449234Z",
     "shell.execute_reply": "2026-01-15T08:38:44.448392Z",
     "shell.execute_reply.started": "2026-01-15T08:38:44.401884Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AST Parser module for Java code.\n",
    "\n",
    "Provides utilities for parsing Java methods into AST representation,\n",
    "extracting node paths, positions, and structural information needed\n",
    "for bug detection.\n",
    "\"\"\"\n",
    "\n",
    "import javalang\n",
    "from typing import Any, Dict, List, Optional, Tuple, Set\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "WRAPPED_CODE_TEMPLATE = \"\"\"public class Dummy {{\n",
    "    {method_code}\n",
    "}}\"\"\"\n",
    "\n",
    "\n",
    "class NodeType(Enum):\n",
    "    \"\"\"Categories of AST nodes for bug detection\"\"\"\n",
    "    DECLARATION = \"declaration\"\n",
    "    EXPRESSION = \"expression\"\n",
    "    STATEMENT = \"statement\"\n",
    "    CONTROL_FLOW = \"control_flow\"\n",
    "    LITERAL = \"literal\"\n",
    "    IDENTIFIER = \"identifier\"\n",
    "    OPERATOR = \"operator\"\n",
    "    TYPE = \"type\"\n",
    "    OTHER = \"other\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ASTNode:\n",
    "    \"\"\"\n",
    "    Represents a node in the AST with all necessary information for bug detection.\n",
    "    \n",
    "    Attributes:\n",
    "        node_type: The class name of the AST node (e.g., 'MethodInvocation')\n",
    "        category: High-level category of the node\n",
    "        label: The value/name associated with the node (if any)\n",
    "        position: (line, column) position in source code\n",
    "        path: List of node types from root to this node\n",
    "        depth: Depth in the AST tree\n",
    "        parent_type: Type of parent node\n",
    "        children_count: Number of direct children\n",
    "        attributes: Additional attributes of the node\n",
    "    \"\"\"\n",
    "    node_type: str\n",
    "    category: NodeType\n",
    "    label: Optional[str] = None\n",
    "    position: Optional[Tuple[int, int]] = None\n",
    "    path: List[str] = field(default_factory=list)\n",
    "    depth: int = 0\n",
    "    parent_type: Optional[str] = None\n",
    "    children_count: int = 0\n",
    "    attributes: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary for serialization\"\"\"\n",
    "        return {\n",
    "            \"node_type\": self.node_type,\n",
    "            \"category\": self.category.value,\n",
    "            \"label\": self.label,\n",
    "            \"position\": self.position,\n",
    "            \"path\": self.path,\n",
    "            \"depth\": self.depth,\n",
    "            \"parent_type\": self.parent_type,\n",
    "            \"children_count\": self.children_count,\n",
    "            \"attributes\": self.attributes\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def path_string(self) -> str:\n",
    "        \"\"\"Get path as string representation\"\"\"\n",
    "        return \" -> \".join(self.path + [self.node_type])\n",
    "\n",
    "\n",
    "@dataclass  \n",
    "class ASTTree:\n",
    "    \"\"\"\n",
    "    Represents a complete AST for a Java method.\n",
    "    \n",
    "    Attributes:\n",
    "        nodes: List of all nodes in the tree (flattened)\n",
    "        root: The root method node\n",
    "        source_code: Original source code\n",
    "        node_by_position: Mapping from position to node for quick lookup\n",
    "    \"\"\"\n",
    "    nodes: List[ASTNode]\n",
    "    root: Optional[Any] = None\n",
    "    source_code: str = \"\"\n",
    "    node_by_position: Dict[Tuple[int, int], ASTNode] = field(default_factory=dict)\n",
    "    \n",
    "    def get_nodes_at_line(self, line: int) -> List[ASTNode]:\n",
    "        \"\"\"Get all nodes at a specific line\"\"\"\n",
    "        return [n for n in self.nodes if n.position and n.position[0] == line]\n",
    "    \n",
    "    def get_nodes_by_type(self, node_type: str) -> List[ASTNode]:\n",
    "        \"\"\"Get all nodes of a specific type\"\"\"\n",
    "        return [n for n in self.nodes if n.node_type == node_type]\n",
    "    \n",
    "    def get_nodes_by_category(self, category: NodeType) -> List[ASTNode]:\n",
    "        \"\"\"Get all nodes of a specific category\"\"\"\n",
    "        return [n for n in self.nodes if n.category == category]\n",
    "\n",
    "\n",
    "class ASTParser:\n",
    "    \"\"\"\n",
    "    Parser for Java methods that extracts AST representation suitable for bug detection.\n",
    "    \n",
    "    Features:\n",
    "    - Parses single Java methods (wraps in dummy class)\n",
    "    - Extracts node types, labels, positions\n",
    "    - Computes AST paths from root to each node\n",
    "    - Categorizes nodes for downstream processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mapping from javalang node types to categories\n",
    "    CATEGORY_MAPPING = {\n",
    "        # Declarations\n",
    "        'MethodDeclaration': NodeType.DECLARATION,\n",
    "        'ConstructorDeclaration': NodeType.DECLARATION,\n",
    "        'VariableDeclarator': NodeType.DECLARATION,\n",
    "        'LocalVariableDeclaration': NodeType.DECLARATION,\n",
    "        'FormalParameter': NodeType.DECLARATION,\n",
    "        'FieldDeclaration': NodeType.DECLARATION,\n",
    "        \n",
    "        # Expressions\n",
    "        'MethodInvocation': NodeType.EXPRESSION,\n",
    "        'Assignment': NodeType.EXPRESSION,\n",
    "        'BinaryOperation': NodeType.EXPRESSION,\n",
    "        'TernaryExpression': NodeType.EXPRESSION,\n",
    "        'Cast': NodeType.EXPRESSION,\n",
    "        'ArraySelector': NodeType.EXPRESSION,\n",
    "        'ArrayCreator': NodeType.EXPRESSION,\n",
    "        'ClassCreator': NodeType.EXPRESSION,\n",
    "        'SuperMethodInvocation': NodeType.EXPRESSION,\n",
    "        'ExplicitConstructorInvocation': NodeType.EXPRESSION,\n",
    "        'LambdaExpression': NodeType.EXPRESSION,\n",
    "        'MethodReference': NodeType.EXPRESSION,\n",
    "        \n",
    "        # Statements\n",
    "        'ReturnStatement': NodeType.STATEMENT,\n",
    "        'BlockStatement': NodeType.STATEMENT,\n",
    "        'StatementExpression': NodeType.STATEMENT,\n",
    "        'ThrowStatement': NodeType.STATEMENT,\n",
    "        'AssertStatement': NodeType.STATEMENT,\n",
    "        'SynchronizedStatement': NodeType.STATEMENT,\n",
    "        \n",
    "        # Control Flow\n",
    "        'IfStatement': NodeType.CONTROL_FLOW,\n",
    "        'ForStatement': NodeType.CONTROL_FLOW,\n",
    "        'WhileStatement': NodeType.CONTROL_FLOW,\n",
    "        'DoStatement': NodeType.CONTROL_FLOW,\n",
    "        'SwitchStatement': NodeType.CONTROL_FLOW,\n",
    "        'SwitchStatementCase': NodeType.CONTROL_FLOW,\n",
    "        'TryStatement': NodeType.CONTROL_FLOW,\n",
    "        'CatchClause': NodeType.CONTROL_FLOW,\n",
    "        'BreakStatement': NodeType.CONTROL_FLOW,\n",
    "        'ContinueStatement': NodeType.CONTROL_FLOW,\n",
    "        'EnhancedForControl': NodeType.CONTROL_FLOW,\n",
    "        \n",
    "        # Literals\n",
    "        'Literal': NodeType.LITERAL,\n",
    "        \n",
    "        # Identifiers/References\n",
    "        'MemberReference': NodeType.IDENTIFIER,\n",
    "        'ReferenceType': NodeType.IDENTIFIER,\n",
    "        'TypeArgument': NodeType.IDENTIFIER,\n",
    "        \n",
    "        # Types\n",
    "        'BasicType': NodeType.TYPE,\n",
    "        'ReferenceType': NodeType.TYPE,\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wrapped_template = WRAPPED_CODE_TEMPLATE\n",
    "    \n",
    "    def _wrap_method(self, method_code: str) -> str:\n",
    "        \"\"\"Wrap method code in a dummy class for parsing\"\"\"\n",
    "        return self.wrapped_template.format(method_code=method_code)\n",
    "    \n",
    "    def _get_category(self, node_type: str) -> NodeType:\n",
    "        \"\"\"Get category for a node type\"\"\"\n",
    "        return self.CATEGORY_MAPPING.get(node_type, NodeType.OTHER)\n",
    "    \n",
    "    def _get_node_label(self, node: Any) -> Optional[str]:\n",
    "        \"\"\"Extract the label/value from an AST node\"\"\"\n",
    "        if hasattr(node, 'name') and node.name:\n",
    "            return node.name\n",
    "        if hasattr(node, 'member') and node.member:\n",
    "            return node.member\n",
    "        if hasattr(node, 'value') and node.value:\n",
    "            return str(node.value)\n",
    "        if hasattr(node, 'operator') and node.operator:\n",
    "            return node.operator\n",
    "        if hasattr(node, 'type') and isinstance(node.type, str):\n",
    "            return node.type\n",
    "        return None\n",
    "    \n",
    "    def _count_children(self, node: Any) -> int:\n",
    "        \"\"\"Count direct children of a node\"\"\"\n",
    "        count = 0\n",
    "        if hasattr(node, 'children'):\n",
    "            for child in node.children:\n",
    "                if child is not None:\n",
    "                    if isinstance(child, list):\n",
    "                        count += len([c for c in child if c is not None])\n",
    "                    else:\n",
    "                        count += 1\n",
    "        return count\n",
    "    \n",
    "    def _extract_attributes(self, node: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Extract additional attributes from a node\"\"\"\n",
    "        attrs = {}\n",
    "        \n",
    "        # Common attributes to extract\n",
    "        attr_names = ['modifiers', 'annotations', 'throws', 'dimensions']\n",
    "        for attr in attr_names:\n",
    "            if hasattr(node, attr) and getattr(node, attr):\n",
    "                value = getattr(node, attr)\n",
    "                if isinstance(value, (list, tuple)):\n",
    "                    attrs[attr] = [str(v) for v in value]\n",
    "                else:\n",
    "                    attrs[attr] = str(value)\n",
    "        \n",
    "        return attrs\n",
    "    \n",
    "    def parse(self, method_code: str, skip_wrapper: bool = True) -> ASTTree:\n",
    "        \"\"\"\n",
    "        Parse Java method code into an ASTTree.\n",
    "        \n",
    "        Args:\n",
    "            method_code: Java method source code\n",
    "            skip_wrapper: Whether to skip the dummy wrapper class nodes\n",
    "            \n",
    "        Returns:\n",
    "            ASTTree containing all parsed nodes\n",
    "        \"\"\"\n",
    "        wrapped = self._wrap_method(method_code)\n",
    "        tree = javalang.parse.parse(wrapped)\n",
    "        \n",
    "        nodes = []\n",
    "        node_by_position = {}\n",
    "        root_method = None\n",
    "        \n",
    "        for path, node in tree:\n",
    "            # Skip primitive/basic types\n",
    "            if isinstance(node, (str, int, float, bool)) or node is None:\n",
    "                continue\n",
    "            \n",
    "            # Skip wrapper class if requested\n",
    "            if skip_wrapper:\n",
    "                if isinstance(node, javalang.tree.CompilationUnit):\n",
    "                    continue\n",
    "                if isinstance(node, javalang.tree.ClassDeclaration) and node.name == 'Dummy':\n",
    "                    continue\n",
    "            \n",
    "            node_type = node.__class__.__name__\n",
    "            category = self._get_category(node_type)\n",
    "            label = self._get_node_label(node)\n",
    "            \n",
    "            # Get position\n",
    "            position = None\n",
    "            if hasattr(node, 'position') and node.position:\n",
    "                position = (node.position.line, node.position.column)\n",
    "            \n",
    "            # Build path from ancestors\n",
    "            path_types = [p.__class__.__name__ for p in path if p is not None]\n",
    "            if skip_wrapper:\n",
    "                # Remove wrapper-related nodes from path\n",
    "                path_types = [p for p in path_types if p not in ['CompilationUnit', 'ClassDeclaration']]\n",
    "            \n",
    "            # Get parent type\n",
    "            parent_type = path_types[-1] if path_types else None\n",
    "            \n",
    "            # Create AST node\n",
    "            ast_node = ASTNode(\n",
    "                node_type=node_type,\n",
    "                category=category,\n",
    "                label=label,\n",
    "                position=position,\n",
    "                path=path_types,\n",
    "                depth=len(path_types),\n",
    "                parent_type=parent_type,\n",
    "                children_count=self._count_children(node),\n",
    "                attributes=self._extract_attributes(node)\n",
    "            )\n",
    "            \n",
    "            nodes.append(ast_node)\n",
    "            \n",
    "            if position:\n",
    "                node_by_position[position] = ast_node\n",
    "            \n",
    "            # Store root method\n",
    "            if isinstance(node, javalang.tree.MethodDeclaration):\n",
    "                root_method = node\n",
    "        \n",
    "        return ASTTree(\n",
    "            nodes=nodes,\n",
    "            root=root_method,\n",
    "            source_code=method_code,\n",
    "            node_by_position=node_by_position\n",
    "        )\n",
    "    \n",
    "    def get_method_declaration(self, method_code: str) -> Optional[Any]:\n",
    "        \"\"\"Extract just the method declaration node\"\"\"\n",
    "        wrapped = self._wrap_method(method_code)\n",
    "        tree = javalang.parse.parse(wrapped)\n",
    "        \n",
    "        for _, node in tree.filter(javalang.tree.MethodDeclaration):\n",
    "            return node\n",
    "        return None\n",
    "    \n",
    "    def get_all_paths(self, method_code: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get all unique AST paths in the method.\n",
    "        \n",
    "        Returns:\n",
    "            List of path strings like \"MethodDeclaration -> BlockStatement -> IfStatement\"\n",
    "        \"\"\"\n",
    "        ast_tree = self.parse(method_code)\n",
    "        return list(set(node.path_string for node in ast_tree.nodes))\n",
    "    \n",
    "    def get_node_sequence(self, method_code: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get linearized sequence of node types (for sequence models).\n",
    "        \n",
    "        Returns:\n",
    "            List of node type names in traversal order\n",
    "        \"\"\"\n",
    "        ast_tree = self.parse(method_code)\n",
    "        return [node.node_type for node in ast_tree.nodes]\n",
    "    \n",
    "    def get_identifiers(self, method_code: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract all identifiers (variables, method names) with their roles.\n",
    "        \n",
    "        Returns:\n",
    "            List of dicts with name, role (declaration/usage/call), and type\n",
    "        \"\"\"\n",
    "        ast_tree = self.parse(method_code)\n",
    "        identifiers = []\n",
    "        \n",
    "        for node in ast_tree.nodes:\n",
    "            if node.label and node.category in [NodeType.DECLARATION, NodeType.IDENTIFIER, NodeType.EXPRESSION]:\n",
    "                role = \"declaration\" if node.category == NodeType.DECLARATION else \"usage\"\n",
    "                if node.node_type == 'MethodInvocation':\n",
    "                    role = \"call\"\n",
    "                    \n",
    "                identifiers.append({\n",
    "                    \"name\": node.label,\n",
    "                    \"role\": role,\n",
    "                    \"type\": node.node_type,\n",
    "                    \"position\": node.position\n",
    "                })\n",
    "        \n",
    "        return identifiers\n",
    "    \n",
    "    def is_valid_method(self, method_code: str) -> bool:\n",
    "        \"\"\"Check if the method code is syntactically valid\"\"\"\n",
    "        try:\n",
    "            self.parse(method_code)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def to_dict(self, method_code: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert method to dictionary representation.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with method info and nested node structure\n",
    "        \"\"\"\n",
    "        method_node = self.get_method_declaration(method_code)\n",
    "        if method_node is None:\n",
    "            return {}\n",
    "        \n",
    "        def node_to_dict(node: Any) -> Any:\n",
    "            if isinstance(node, (str, int, float, bool)) or node is None:\n",
    "                return node\n",
    "            if isinstance(node, list):\n",
    "                return [node_to_dict(x) for x in node]\n",
    "            if not hasattr(node, '__dict__'):\n",
    "                return str(node)\n",
    "            \n",
    "            result = {\"type\": node.__class__.__name__}\n",
    "            for attr, value in node.__dict__.items():\n",
    "                if attr == \"position\":\n",
    "                    if value:\n",
    "                        result[\"position\"] = {\"line\": value.line, \"column\": value.column}\n",
    "                    continue\n",
    "                if value is not None:\n",
    "                    result[attr] = node_to_dict(value)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        return {\n",
    "            \"name\": method_node.name,\n",
    "            \"return_type\": node_to_dict(method_node.return_type),\n",
    "            \"parameters\": node_to_dict(method_node.parameters),\n",
    "            \"throws\": node_to_dict(method_node.throws),\n",
    "            \"body\": node_to_dict(method_node.body)\n",
    "        }\n",
    "\n",
    "\n",
    "# Utility functions for quick access\n",
    "def parse_java_method(method_code: str) -> ASTTree:\n",
    "    \"\"\"Quick function to parse a Java method\"\"\"\n",
    "    parser = ASTParser()\n",
    "    return parser.parse(method_code)\n",
    "\n",
    "\n",
    "def get_ast_paths(method_code: str) -> List[str]:\n",
    "    \"\"\"Quick function to get all AST paths\"\"\"\n",
    "    parser = ASTParser()\n",
    "    return parser.get_all_paths(method_code)\n",
    "\n",
    "\n",
    "def is_valid_java_method(method_code: str) -> bool:\n",
    "    \"\"\"Quick function to validate Java method syntax\"\"\"\n",
    "    parser = ASTParser()\n",
    "    return parser.is_valid_method(method_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GumTreeDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:38:44.450957Z",
     "iopub.status.busy": "2026-01-15T08:38:44.450694Z",
     "iopub.status.idle": "2026-01-15T08:38:44.504524Z",
     "shell.execute_reply": "2026-01-15T08:38:44.503629Z",
     "shell.execute_reply.started": "2026-01-15T08:38:44.450930Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GumTree Diff module for computing AST differences between buggy and fixed code.\n",
    "\n",
    "Uses GumTree CLI to extract edit actions (insert, delete, update, move)\n",
    "that transform the buggy code into the fixed version.\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import platform\n",
    "import tempfile\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class EditType(Enum):\n",
    "    \"\"\"Types of edit actions in AST diff\"\"\"\n",
    "    INSERT = \"insert\"\n",
    "    DELETE = \"delete\"\n",
    "    UPDATE = \"update\"\n",
    "    MOVE = \"move\"\n",
    "    \n",
    "    @classmethod\n",
    "    def from_string(cls, s: str) -> \"EditType\":\n",
    "        \"\"\"Convert string to EditType\"\"\"\n",
    "        mapping = {\n",
    "            \"insert-node\": cls.INSERT,\n",
    "            \"insert-tree\": cls.INSERT,\n",
    "            \"delete-node\": cls.DELETE,\n",
    "            \"delete-tree\": cls.DELETE,\n",
    "            \"update-node\": cls.UPDATE,\n",
    "            \"move-tree\": cls.MOVE,\n",
    "            \"insert\": cls.INSERT,\n",
    "            \"delete\": cls.DELETE,\n",
    "            \"update\": cls.UPDATE,\n",
    "            \"move\": cls.MOVE,\n",
    "        }\n",
    "        return mapping.get(s.lower(), cls.UPDATE)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ASTDiffNode:\n",
    "    \"\"\"\n",
    "    Represents a node involved in an edit action.\n",
    "    \n",
    "    Attributes:\n",
    "        node_type: Type of the AST node (e.g., 'SimpleName', 'MethodInvocation')\n",
    "        label: Value/name of the node\n",
    "        position: (start_pos, end_pos) in source code\n",
    "        line: Line number in source\n",
    "    \"\"\"\n",
    "    node_type: str\n",
    "    label: Optional[str] = None\n",
    "    position: Optional[Tuple[int, int]] = None\n",
    "    line: Optional[int] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"node_type\": self.node_type,\n",
    "            \"label\": self.label,\n",
    "            \"position\": self.position,\n",
    "            \"line\": self.line\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EditAction:\n",
    "    \"\"\"\n",
    "    Represents a single edit action from GumTree diff.\n",
    "    \n",
    "    Attributes:\n",
    "        action_type: Type of edit (insert, delete, update, move)\n",
    "        node: The AST node being edited\n",
    "        parent: Parent node (for inserts)\n",
    "        at_position: Position in parent's children (for inserts)\n",
    "        old_value: Original value (for updates)\n",
    "        new_value: New value (for updates)\n",
    "        raw_data: Original raw data from GumTree\n",
    "    \"\"\"\n",
    "    action_type: EditType\n",
    "    node: ASTDiffNode\n",
    "    parent: Optional[ASTDiffNode] = None\n",
    "    at_position: Optional[int] = None\n",
    "    old_value: Optional[str] = None\n",
    "    new_value: Optional[str] = None\n",
    "    raw_data: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"action_type\": self.action_type.value,\n",
    "            \"node\": self.node.to_dict(),\n",
    "            \"parent\": self.parent.to_dict() if self.parent else None,\n",
    "            \"at_position\": self.at_position,\n",
    "            \"old_value\": self.old_value,\n",
    "            \"new_value\": self.new_value\n",
    "        }\n",
    "    \n",
    "    def to_string(self) -> str:\n",
    "        \"\"\"Get human-readable string representation\"\"\"\n",
    "        parts = [f\"[{self.action_type.value.upper()}]\"]\n",
    "        parts.append(self.node.node_type)\n",
    "        \n",
    "        if self.node.label:\n",
    "            parts.append(f\"'{self.node.label}'\")\n",
    "        \n",
    "        if self.action_type == EditType.UPDATE and self.new_value:\n",
    "            parts.append(f\"-> '{self.new_value}'\")\n",
    "        \n",
    "        if self.parent:\n",
    "            parts.append(f\"@ {self.parent.node_type}\")\n",
    "            \n",
    "        return \" \".join(parts)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiffResult:\n",
    "    \"\"\"\n",
    "    Result of computing AST diff between two code versions.\n",
    "    \n",
    "    Attributes:\n",
    "        actions: List of edit actions\n",
    "        src_nodes: Number of nodes in source AST\n",
    "        dst_nodes: Number of nodes in destination AST\n",
    "        edit_distance: Total number of edits\n",
    "        mappings: Node mappings between source and destination\n",
    "    \"\"\"\n",
    "    actions: List[EditAction]\n",
    "    src_nodes: int = 0\n",
    "    dst_nodes: int = 0\n",
    "    edit_distance: int = 0\n",
    "    mappings: List[Tuple[str, str]] = field(default_factory=list)\n",
    "    raw_output: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    @property\n",
    "    def has_changes(self) -> bool:\n",
    "        return len(self.actions) > 0\n",
    "    \n",
    "    @property\n",
    "    def insert_count(self) -> int:\n",
    "        return sum(1 for a in self.actions if a.action_type == EditType.INSERT)\n",
    "    \n",
    "    @property\n",
    "    def delete_count(self) -> int:\n",
    "        return sum(1 for a in self.actions if a.action_type == EditType.DELETE)\n",
    "    \n",
    "    @property\n",
    "    def update_count(self) -> int:\n",
    "        return sum(1 for a in self.actions if a.action_type == EditType.UPDATE)\n",
    "    \n",
    "    @property\n",
    "    def move_count(self) -> int:\n",
    "        return sum(1 for a in self.actions if a.action_type == EditType.MOVE)\n",
    "    \n",
    "    def get_actions_by_type(self, action_type: EditType) -> List[EditAction]:\n",
    "        return [a for a in self.actions if a.action_type == action_type]\n",
    "    \n",
    "    def get_edited_node_types(self) -> List[str]:\n",
    "        \"\"\"Get unique node types that were edited\"\"\"\n",
    "        return list(set(a.node.node_type for a in self.actions))\n",
    "    \n",
    "    def get_deleted_nodes(self) -> List[ASTDiffNode]:\n",
    "        \"\"\"Get all nodes that were deleted (these are the buggy parts)\"\"\"\n",
    "        return [a.node for a in self.actions if a.action_type == EditType.DELETE]\n",
    "    \n",
    "    def get_inserted_nodes(self) -> List[ASTDiffNode]:\n",
    "        \"\"\"Get all nodes that were inserted (these are the fixes)\"\"\"\n",
    "        return [a.node for a in self.actions if a.action_type == EditType.INSERT]\n",
    "    \n",
    "    def to_string(self) -> str:\n",
    "        \"\"\"Get human-readable diff summary\"\"\"\n",
    "        lines = [\n",
    "            f\"Edit Actions ({len(self.actions)} total):\",\n",
    "            f\"  - Inserts: {self.insert_count}\",\n",
    "            f\"  - Deletes: {self.delete_count}\",\n",
    "            f\"  - Updates: {self.update_count}\",\n",
    "            f\"  - Moves: {self.move_count}\",\n",
    "            \"\",\n",
    "            \"Actions:\"\n",
    "        ]\n",
    "        for action in self.actions:\n",
    "            lines.append(f\"  {action.to_string()}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "class GumTreeDiff:\n",
    "    \"\"\"\n",
    "    Wrapper for GumTree CLI to compute AST differences between Java code versions.\n",
    "    \n",
    "    GumTree supports the following edit actions:\n",
    "    - insert: Add new node\n",
    "    - delete: Remove node\n",
    "    - update: Change node value (e.g., rename variable)\n",
    "    - move: Move node to different position\n",
    "    \n",
    "    Usage:\n",
    "        gt = GumTreeDiff()\n",
    "        result = gt.diff(buggy_code, fixed_code)\n",
    "        for action in result.actions:\n",
    "            print(action.to_string())\n",
    "    \"\"\"\n",
    "    \n",
    "    WRAPPED_CODE_TEMPLATE = \"\"\"public class Dummy {{\n",
    "    {method_code}\n",
    "}}\"\"\"\n",
    "    \n",
    "    # Default paths to look for GumTree installation\n",
    "    DEFAULT_PATHS_WINDOWS = [\n",
    "        'E:\\\\CodeBuggy\\\\gumtree-4.0.0-beta4\\\\bin\\\\gumtree.bat',\n",
    "        'E:\\\\CodeBuggy\\\\gumtree-4.0.0-beta6\\\\bin\\\\gumtree.bat',\n",
    "        '.\\\\gumtree\\\\bin\\\\gumtree.bat',\n",
    "        'gumtree.bat',\n",
    "        'gumtree'\n",
    "    ]\n",
    "    \n",
    "    DEFAULT_PATHS_UNIX = [\n",
    "        './gumtree/bin/gumtree',\n",
    "        '/usr/local/bin/gumtree',\n",
    "        'gumtree'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, gumtree_path: str = None, timeout: int = 30):\n",
    "        \"\"\"\n",
    "        Initialize GumTree wrapper.\n",
    "        \n",
    "        Args:\n",
    "            gumtree_path: Path to GumTree executable. Auto-detected if None.\n",
    "            timeout: Timeout in seconds for GumTree commands.\n",
    "        \"\"\"\n",
    "        self.is_windows = platform.system() == 'Windows'\n",
    "        self.timeout = timeout\n",
    "        \n",
    "        if gumtree_path is None:\n",
    "            gumtree_path = self._auto_detect_path()\n",
    "        \n",
    "        self.gumtree_path = gumtree_path\n",
    "        self._validated = False\n",
    "    \n",
    "    def _auto_detect_path(self) -> str:\n",
    "        \"\"\"Auto-detect GumTree installation path\"\"\"\n",
    "        paths = self.DEFAULT_PATHS_WINDOWS if self.is_windows else self.DEFAULT_PATHS_UNIX\n",
    "        \n",
    "        for path in paths:\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "        \n",
    "        # Return default and let validation fail with helpful message\n",
    "        return 'gumtree.bat' if self.is_windows else 'gumtree'\n",
    "    \n",
    "    def validate(self) -> bool:\n",
    "        \"\"\"Validate that GumTree is properly installed and working\"\"\"\n",
    "        if self._validated:\n",
    "            return True\n",
    "            \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [self.gumtree_path, \"--version\"],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                encoding='utf-8',\n",
    "                errors='replace',\n",
    "                timeout=10,\n",
    "                shell=self.is_windows\n",
    "            )\n",
    "            self._validated = result.returncode == 0\n",
    "            return self._validated\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def _wrap_as_class(self, method_code: str) -> str:\n",
    "        \"\"\"Wrap method code in a dummy class for parsing\"\"\"\n",
    "        return self.WRAPPED_CODE_TEMPLATE.format(method_code=method_code)\n",
    "    \n",
    "    def _parse_tree_string(self, tree_str: str) -> ASTDiffNode:\n",
    "        \"\"\"Parse GumTree tree string format into ASTDiffNode\"\"\"\n",
    "        # Format: \"NodeType: label [pos, len]\" or \"NodeType [pos, len]\"\n",
    "        parts = tree_str.strip()\n",
    "        \n",
    "        node_type = parts\n",
    "        label = None\n",
    "        position = None\n",
    "        \n",
    "        # Extract position if present\n",
    "        if '[' in parts and ']' in parts:\n",
    "            pos_start = parts.rfind('[')\n",
    "            pos_end = parts.rfind(']')\n",
    "            pos_str = parts[pos_start+1:pos_end]\n",
    "            parts = parts[:pos_start].strip()\n",
    "            \n",
    "            try:\n",
    "                pos_parts = pos_str.split(',')\n",
    "                if len(pos_parts) >= 2:\n",
    "                    position = (int(pos_parts[0].strip()), int(pos_parts[1].strip()))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Extract label if present\n",
    "        if ':' in parts:\n",
    "            idx = parts.index(':')\n",
    "            node_type = parts[:idx].strip()\n",
    "            label = parts[idx+1:].strip()\n",
    "        else:\n",
    "            node_type = parts.strip()\n",
    "        \n",
    "        return ASTDiffNode(\n",
    "            node_type=node_type,\n",
    "            label=label,\n",
    "            position=position\n",
    "        )\n",
    "    \n",
    "    def _parse_action(self, action_data: Dict[str, Any]) -> EditAction:\n",
    "        \"\"\"Parse a single action from GumTree JSON output\"\"\"\n",
    "        action_type = EditType.from_string(action_data.get(\"action\", \"update\"))\n",
    "        \n",
    "        # Parse the main node\n",
    "        tree_str = action_data.get(\"tree\", \"\")\n",
    "        node = self._parse_tree_string(tree_str)\n",
    "        \n",
    "        # Parse parent if present\n",
    "        parent = None\n",
    "        parent_str = action_data.get(\"parent\", \"\")\n",
    "        if parent_str:\n",
    "            parent = self._parse_tree_string(parent_str)\n",
    "        \n",
    "        # Extract position info\n",
    "        at_position = action_data.get(\"at\")\n",
    "        \n",
    "        return EditAction(\n",
    "            action_type=action_type,\n",
    "            node=node,\n",
    "            parent=parent,\n",
    "            at_position=at_position,\n",
    "            raw_data=action_data\n",
    "        )\n",
    "    \n",
    "    def diff(self, src_code: str, dst_code: str, wrap: bool = True) -> DiffResult:\n",
    "        \"\"\"\n",
    "        Compute AST diff between source and destination code.\n",
    "        \n",
    "        Args:\n",
    "            src_code: Source code (buggy version)\n",
    "            dst_code: Destination code (fixed version)\n",
    "            wrap: Whether to wrap code in dummy class (for method-level code)\n",
    "            \n",
    "        Returns:\n",
    "            DiffResult containing all edit actions\n",
    "        \"\"\"\n",
    "        if wrap:\n",
    "            src_code = self._wrap_as_class(src_code)\n",
    "            dst_code = self._wrap_as_class(dst_code)\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            src_file = Path(tmpdir) / \"Src.java\"\n",
    "            dst_file = Path(tmpdir) / \"Dst.java\"\n",
    "            \n",
    "            src_file.write_text(src_code, encoding='utf-8')\n",
    "            dst_file.write_text(dst_code, encoding='utf-8')\n",
    "            \n",
    "            try:\n",
    "                # Run GumTree diff command\n",
    "                cmd = [\n",
    "                    self.gumtree_path, \"textdiff\",\n",
    "                    \"-f\", \"json\",\n",
    "                    str(src_file), str(dst_file)\n",
    "                ]\n",
    "                \n",
    "                result = subprocess.run(\n",
    "                    cmd,\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    encoding='utf-8',\n",
    "                    errors='replace',\n",
    "                    timeout=self.timeout,\n",
    "                    shell=self.is_windows\n",
    "                )\n",
    "                \n",
    "                if result.returncode != 0:\n",
    "                    print(f\"GumTree error: {result.stderr}\")\n",
    "                    return DiffResult(actions=[])\n",
    "                \n",
    "                # Parse JSON output\n",
    "                output = json.loads(result.stdout)\n",
    "                \n",
    "                # Parse actions\n",
    "                actions = []\n",
    "                for action_data in output.get(\"actions\", []):\n",
    "                    actions.append(self._parse_action(action_data))\n",
    "                \n",
    "                return DiffResult(\n",
    "                    actions=actions,\n",
    "                    raw_output=output\n",
    "                )\n",
    "                \n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(f\"GumTree timeout after {self.timeout}s\")\n",
    "                return DiffResult(actions=[])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Failed to parse GumTree output: {e}\")\n",
    "                return DiffResult(actions=[])\n",
    "            except Exception as e:\n",
    "                print(f\"Error running GumTree: {e}\")\n",
    "                return DiffResult(actions=[])\n",
    "    \n",
    "    def get_edit_script(self, src_code: str, dst_code: str, wrap: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Get human-readable edit script.\n",
    "        \n",
    "        Returns:\n",
    "            Text description of all edits\n",
    "        \"\"\"\n",
    "        if wrap:\n",
    "            src_code = self._wrap_as_class(src_code)\n",
    "            dst_code = self._wrap_as_class(dst_code)\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            src_file = Path(tmpdir) / \"Src.java\"\n",
    "            dst_file = Path(tmpdir) / \"Dst.java\"\n",
    "            \n",
    "            src_file.write_text(src_code, encoding='utf-8')\n",
    "            dst_file.write_text(dst_code, encoding='utf-8')\n",
    "            \n",
    "            try:\n",
    "                cmd = [\n",
    "                    self.gumtree_path, \"textdiff\",\n",
    "                    \"-f\", \"text\",\n",
    "                    str(src_file), str(dst_file)\n",
    "                ]\n",
    "                \n",
    "                result = subprocess.run(\n",
    "                    cmd,\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    encoding='utf-8',\n",
    "                    errors='replace',\n",
    "                    timeout=self.timeout,\n",
    "                    shell=self.is_windows\n",
    "                )\n",
    "                \n",
    "                return result.stdout if result.returncode == 0 else \"\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                return \"\"\n",
    "    \n",
    "    def get_mappings(self, src_code: str, dst_code: str, wrap: bool = True) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get node mappings between source and destination ASTs.\n",
    "        \n",
    "        Returns:\n",
    "            List of mappings showing which nodes correspond to each other\n",
    "        \"\"\"\n",
    "        if wrap:\n",
    "            src_code = self._wrap_as_class(src_code)\n",
    "            dst_code = self._wrap_as_class(dst_code)\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            src_file = Path(tmpdir) / \"Src.java\"\n",
    "            dst_file = Path(tmpdir) / \"Dst.java\"\n",
    "            \n",
    "            src_file.write_text(src_code, encoding='utf-8')\n",
    "            dst_file.write_text(dst_code, encoding='utf-8')\n",
    "            \n",
    "            try:\n",
    "                cmd = [\n",
    "                    self.gumtree_path, \"textdiff\",\n",
    "                    \"-f\", \"json\",\n",
    "                    str(src_file), str(dst_file)\n",
    "                ]\n",
    "                \n",
    "                result = subprocess.run(\n",
    "                    cmd,\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    encoding='utf-8',\n",
    "                    errors='replace',\n",
    "                    timeout=self.timeout,\n",
    "                    shell=self.is_windows\n",
    "                )\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    output = json.loads(result.stdout)\n",
    "                    return output.get(\"matches\", [])\n",
    "                return []\n",
    "                \n",
    "            except Exception:\n",
    "                return []\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def compute_diff(buggy_code: str, fixed_code: str, gumtree_path: str = None) -> DiffResult:\n",
    "    \"\"\"Quick function to compute diff between buggy and fixed code\"\"\"\n",
    "    gt = GumTreeDiff(gumtree_path=gumtree_path)\n",
    "    return gt.diff(buggy_code, fixed_code)\n",
    "\n",
    "\n",
    "def get_edit_actions(buggy_code: str, fixed_code: str, gumtree_path: str = None) -> List[EditAction]:\n",
    "    \"\"\"Quick function to get list of edit actions\"\"\"\n",
    "    result = compute_diff(buggy_code, fixed_code, gumtree_path)\n",
    "    return result.actions\n",
    "\n",
    "\n",
    "def format_diff_for_model(buggy_code: str, fixed_code: str, gumtree_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Format diff as text suitable for model input.\n",
    "    \n",
    "    Returns:\n",
    "        String with each action on a line: [ACTION_TYPE] node_type: label\n",
    "    \"\"\"\n",
    "    result = compute_diff(buggy_code, fixed_code, gumtree_path)\n",
    "    return \"\\n\".join(action.to_string() for action in result.actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:38:44.505888Z",
     "iopub.status.busy": "2026-01-15T08:38:44.505612Z",
     "iopub.status.idle": "2026-01-15T08:38:44.556296Z",
     "shell.execute_reply": "2026-01-15T08:38:44.555323Z",
     "shell.execute_reply.started": "2026-01-15T08:38:44.505864Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feature Extractor module for extracting supervision signals from AST diffs.\n",
    "\n",
    "Extracts features for:\n",
    "1. Buggy region masking - identify which tokens/nodes are buggy\n",
    "2. Bug localization - pinpoint exact location of bugs\n",
    "3. Contrastive learning - contrast buggy vs fixed representations\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional, Tuple, Set\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "\n",
    "# from .ast_parser import ASTParser, ASTTree, ASTNode, NodeType\n",
    "# from .gumtree_diff import GumTreeDiff, DiffResult, EditAction, EditType, ASTDiffNode\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BuggyRegionMask:\n",
    "    \"\"\"\n",
    "    Represents a mask over the buggy code indicating which parts are buggy.\n",
    "    \n",
    "    Attributes:\n",
    "        token_mask: List of 0/1 for each token (1 = buggy)\n",
    "        char_mask: List of 0/1 for each character\n",
    "        line_mask: Dict mapping line number to buggy status\n",
    "        node_mask: List of (node_type, is_buggy) pairs\n",
    "        buggy_spans: List of (start_idx, end_idx) character spans\n",
    "    \"\"\"\n",
    "    token_mask: List[int] = field(default_factory=list)\n",
    "    char_mask: List[int] = field(default_factory=list)\n",
    "    line_mask: Dict[int, int] = field(default_factory=dict)\n",
    "    node_mask: List[Tuple[str, int]] = field(default_factory=list)\n",
    "    buggy_spans: List[Tuple[int, int]] = field(default_factory=list)\n",
    "    \n",
    "    def get_buggy_lines(self) -> List[int]:\n",
    "        \"\"\"Get list of buggy line numbers\"\"\"\n",
    "        return [line for line, is_buggy in self.line_mask.items() if is_buggy == 1]\n",
    "    \n",
    "    def get_buggy_percentage(self) -> float:\n",
    "        \"\"\"Get percentage of tokens that are buggy\"\"\"\n",
    "        if not self.token_mask:\n",
    "            return 0.0\n",
    "        return sum(self.token_mask) / len(self.token_mask)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"token_mask\": self.token_mask,\n",
    "            \"line_mask\": self.line_mask,\n",
    "            \"buggy_spans\": self.buggy_spans,\n",
    "            \"buggy_lines\": self.get_buggy_lines(),\n",
    "            \"buggy_percentage\": self.get_buggy_percentage()\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BugLocalization:\n",
    "    \"\"\"\n",
    "    Fine-grained bug localization information.\n",
    "    \n",
    "    Attributes:\n",
    "        buggy_nodes: List of AST nodes that are buggy\n",
    "        buggy_tokens: List of buggy token strings\n",
    "        buggy_lines: Line numbers containing bugs\n",
    "        fix_nodes: List of AST nodes that were added as fixes\n",
    "        edit_operations: List of edit operation descriptions\n",
    "        bug_type: Inferred type of bug\n",
    "    \"\"\"\n",
    "    buggy_nodes: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    buggy_tokens: List[str] = field(default_factory=list)\n",
    "    buggy_lines: List[int] = field(default_factory=list)\n",
    "    fix_nodes: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    edit_operations: List[str] = field(default_factory=list)\n",
    "    bug_type: str = \"unknown\"\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"buggy_nodes\": self.buggy_nodes,\n",
    "            \"buggy_tokens\": self.buggy_tokens,\n",
    "            \"buggy_lines\": self.buggy_lines,\n",
    "            \"fix_nodes\": self.fix_nodes,\n",
    "            \"edit_operations\": self.edit_operations,\n",
    "            \"bug_type\": self.bug_type\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ContrastiveFeatures:\n",
    "    \"\"\"\n",
    "    Features for contrastive learning between buggy and fixed code.\n",
    "    \n",
    "    Attributes:\n",
    "        buggy_ast_seq: AST node sequence for buggy code\n",
    "        fixed_ast_seq: AST node sequence for fixed code\n",
    "        buggy_tokens: Token sequence for buggy code\n",
    "        fixed_tokens: Token sequence for fixed code\n",
    "        common_nodes: Nodes that appear in both versions\n",
    "        diff_nodes: Nodes that differ between versions\n",
    "        edit_path: Sequence of edits to transform buggy to fixed\n",
    "    \"\"\"\n",
    "    buggy_ast_seq: List[str] = field(default_factory=list)\n",
    "    fixed_ast_seq: List[str] = field(default_factory=list)\n",
    "    buggy_tokens: List[str] = field(default_factory=list)\n",
    "    fixed_tokens: List[str] = field(default_factory=list)\n",
    "    common_nodes: List[str] = field(default_factory=list)\n",
    "    diff_nodes: List[str] = field(default_factory=list)\n",
    "    edit_path: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"buggy_ast_seq\": self.buggy_ast_seq,\n",
    "            \"fixed_ast_seq\": self.fixed_ast_seq,\n",
    "            \"buggy_tokens\": self.buggy_tokens,\n",
    "            \"fixed_tokens\": self.fixed_tokens,\n",
    "            \"common_nodes\": self.common_nodes,\n",
    "            \"diff_nodes\": self.diff_nodes,\n",
    "            \"edit_path\": self.edit_path\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiffFeatures:\n",
    "    \"\"\"\n",
    "    Complete feature set extracted from AST diff.\n",
    "    \n",
    "    Contains all features needed for bug detection:\n",
    "    - Region mask for identifying buggy parts\n",
    "    - Localization for pinpointing bugs\n",
    "    - Contrastive features for learning representations\n",
    "    \"\"\"\n",
    "    mask: BuggyRegionMask\n",
    "    localization: BugLocalization\n",
    "    contrastive: ContrastiveFeatures\n",
    "    diff_result: DiffResult\n",
    "    \n",
    "    # Summary statistics\n",
    "    num_edits: int = 0\n",
    "    edit_types: Dict[str, int] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"mask\": self.mask.to_dict(),\n",
    "            \"localization\": self.localization.to_dict(),\n",
    "            \"contrastive\": self.contrastive.to_dict(),\n",
    "            \"num_edits\": self.num_edits,\n",
    "            \"edit_types\": self.edit_types\n",
    "        }\n",
    "\n",
    "\n",
    "class DiffFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extracts features from AST diffs for bug detection.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Parse both buggy and fixed code to AST\n",
    "    2. Compute AST diff using GumTree\n",
    "    3. Extract features for:\n",
    "       - Buggy region masking\n",
    "       - Bug localization  \n",
    "       - Contrastive learning\n",
    "    \n",
    "    Usage:\n",
    "        extractor = DiffFeatureExtractor()\n",
    "        features = extractor.extract(buggy_code, fixed_code)\n",
    "        \n",
    "        # Use for training\n",
    "        mask = features.mask.token_mask  # [0, 0, 1, 1, 0, ...]\n",
    "        buggy_lines = features.localization.buggy_lines  # [5, 6, 7]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Bug type patterns based on edit actions\n",
    "    BUG_TYPE_PATTERNS = {\n",
    "        \"null_check\": [\"NullLiteral\", \"IfStatement\", \"null\"],\n",
    "        \"off_by_one\": [\"BinaryOperation\", \"<\", \">\", \"<=\", \">=\", \"InfixExpression\"],\n",
    "        \"wrong_operator\": [\"BinaryOperation\", \"InfixExpression\"],\n",
    "        \"missing_return\": [\"ReturnStatement\"],\n",
    "        \"wrong_variable\": [\"SimpleName\", \"MemberReference\"],\n",
    "        \"missing_condition\": [\"IfStatement\", \"WhileStatement\", \"ForStatement\"],\n",
    "        \"type_error\": [\"Cast\", \"TypeLiteral\", \"ReferenceType\"],\n",
    "        \"api_misuse\": [\"MethodInvocation\", \"SuperMethodInvocation\"],\n",
    "    }\n",
    "    \n",
    "    def __init__(self, gumtree_path: str = None):\n",
    "        \"\"\"\n",
    "        Initialize feature extractor.\n",
    "        \n",
    "        Args:\n",
    "            gumtree_path: Path to GumTree executable\n",
    "        \"\"\"\n",
    "        self.ast_parser = ASTParser()\n",
    "        self.gumtree = GumTreeDiff(gumtree_path=gumtree_path)\n",
    "    \n",
    "    def _tokenize(self, code: str) -> List[str]:\n",
    "        \"\"\"Simple tokenization for Java code\"\"\"\n",
    "        # Split on whitespace and keep operators/punctuation as tokens\n",
    "        pattern = r'(\\s+|[{}()\\[\\];,.<>=!&|+\\-*/:])'\n",
    "        tokens = re.split(pattern, code)\n",
    "        return [t for t in tokens if t.strip()]\n",
    "    \n",
    "    def _get_token_positions(self, code: str) -> List[Tuple[int, int, str]]:\n",
    "        \"\"\"Get (start, end, token) for each token in code\"\"\"\n",
    "        positions = []\n",
    "        pattern = r'(\\s+|[{}()\\[\\];,.<>=!&|+\\-*/:])'\n",
    "        \n",
    "        pos = 0\n",
    "        for match in re.split(pattern, code):\n",
    "            if match.strip():\n",
    "                positions.append((pos, pos + len(match), match))\n",
    "            pos += len(match)\n",
    "        \n",
    "        return positions\n",
    "    \n",
    "    def _infer_bug_type(self, actions: List[EditAction]) -> str:\n",
    "        \"\"\"Infer bug type based on edit actions\"\"\"\n",
    "        action_info = set()\n",
    "        \n",
    "        for action in actions:\n",
    "            action_info.add(action.node.node_type)\n",
    "            if action.node.label:\n",
    "                action_info.add(action.node.label)\n",
    "        \n",
    "        # Check each bug type pattern\n",
    "        for bug_type, patterns in self.BUG_TYPE_PATTERNS.items():\n",
    "            matches = sum(1 for p in patterns if p in action_info)\n",
    "            if matches >= 2:\n",
    "                return bug_type\n",
    "        \n",
    "        # Default based on edit types\n",
    "        if any(a.action_type == EditType.DELETE for a in actions):\n",
    "            return \"unnecessary_code\"\n",
    "        if any(a.action_type == EditType.INSERT for a in actions):\n",
    "            return \"missing_code\"\n",
    "        if any(a.action_type == EditType.UPDATE for a in actions):\n",
    "            return \"wrong_value\"\n",
    "        \n",
    "        return \"unknown\"\n",
    "    \n",
    "    def extract_mask(self, buggy_code: str, diff_result: DiffResult) -> BuggyRegionMask:\n",
    "        \"\"\"\n",
    "        Extract buggy region mask from diff result.\n",
    "        \n",
    "        Creates masks at different granularities:\n",
    "        - Token level: which tokens are buggy\n",
    "        - Character level: which characters are buggy\n",
    "        - Line level: which lines are buggy\n",
    "        \"\"\"\n",
    "        tokens = self._tokenize(buggy_code)\n",
    "        token_positions = self._get_token_positions(buggy_code)\n",
    "        lines = buggy_code.split('\\n')\n",
    "        \n",
    "        # Initialize masks\n",
    "        token_mask = [0] * len(tokens)\n",
    "        char_mask = [0] * len(buggy_code)\n",
    "        line_mask = {i: 0 for i in range(1, len(lines) + 1)}\n",
    "        node_mask = []\n",
    "        buggy_spans = []\n",
    "        \n",
    "        # Get deleted/updated nodes (these mark buggy regions)\n",
    "        buggy_actions = [a for a in diff_result.actions \n",
    "                        if a.action_type in [EditType.DELETE, EditType.UPDATE]]\n",
    "        \n",
    "        for action in buggy_actions:\n",
    "            node = action.node\n",
    "            \n",
    "            # If we have position info\n",
    "            if node.position:\n",
    "                start_pos, length = node.position\n",
    "                end_pos = start_pos + length\n",
    "                \n",
    "                # Mark character mask\n",
    "                for i in range(max(0, start_pos), min(end_pos, len(char_mask))):\n",
    "                    char_mask[i] = 1\n",
    "                \n",
    "                buggy_spans.append((start_pos, end_pos))\n",
    "                \n",
    "                # Mark token mask based on position overlap\n",
    "                for idx, (t_start, t_end, token) in enumerate(token_positions):\n",
    "                    if t_start < end_pos and t_end > start_pos:\n",
    "                        token_mask[idx] = 1\n",
    "            \n",
    "            # If we have label, try to find it in code\n",
    "            elif node.label:\n",
    "                label = node.label\n",
    "                for idx, (t_start, t_end, token) in enumerate(token_positions):\n",
    "                    if token == label:\n",
    "                        token_mask[idx] = 1\n",
    "                        for i in range(t_start, t_end):\n",
    "                            if i < len(char_mask):\n",
    "                                char_mask[i] = 1\n",
    "            \n",
    "            # Track node types\n",
    "            node_mask.append((node.node_type, 1))\n",
    "        \n",
    "        # Compute line mask from character mask\n",
    "        char_pos = 0\n",
    "        for line_num, line in enumerate(lines, 1):\n",
    "            line_end = char_pos + len(line)\n",
    "            if any(char_mask[i] for i in range(char_pos, min(line_end, len(char_mask)))):\n",
    "                line_mask[line_num] = 1\n",
    "            char_pos = line_end + 1  # +1 for newline\n",
    "        \n",
    "        return BuggyRegionMask(\n",
    "            token_mask=token_mask,\n",
    "            char_mask=char_mask,\n",
    "            line_mask=line_mask,\n",
    "            node_mask=node_mask,\n",
    "            buggy_spans=buggy_spans\n",
    "        )\n",
    "    \n",
    "    def extract_localization(self, buggy_code: str, fixed_code: str, \n",
    "                            diff_result: DiffResult) -> BugLocalization:\n",
    "        \"\"\"\n",
    "        Extract bug localization information.\n",
    "        \n",
    "        Identifies:\n",
    "        - Which AST nodes are buggy\n",
    "        - Which tokens are buggy\n",
    "        - Which lines contain bugs\n",
    "        - What fixes were applied\n",
    "        \"\"\"\n",
    "        buggy_nodes = []\n",
    "        fix_nodes = []\n",
    "        buggy_tokens = []\n",
    "        buggy_lines = set()\n",
    "        edit_operations = []\n",
    "        \n",
    "        for action in diff_result.actions:\n",
    "            node = action.node\n",
    "            \n",
    "            # Build node info\n",
    "            node_info = {\n",
    "                \"type\": node.node_type,\n",
    "                \"label\": node.label,\n",
    "                \"position\": node.position,\n",
    "                \"action\": action.action_type.value\n",
    "            }\n",
    "            \n",
    "            if action.action_type in [EditType.DELETE, EditType.UPDATE]:\n",
    "                buggy_nodes.append(node_info)\n",
    "                if node.label:\n",
    "                    buggy_tokens.append(node.label)\n",
    "                if node.line:\n",
    "                    buggy_lines.add(node.line)\n",
    "            \n",
    "            if action.action_type in [EditType.INSERT, EditType.UPDATE]:\n",
    "                fix_nodes.append(node_info)\n",
    "            \n",
    "            # Create operation description\n",
    "            edit_operations.append(action.to_string())\n",
    "        \n",
    "        # Infer bug type\n",
    "        bug_type = self._infer_bug_type(diff_result.actions)\n",
    "        \n",
    "        return BugLocalization(\n",
    "            buggy_nodes=buggy_nodes,\n",
    "            buggy_tokens=list(set(buggy_tokens)),\n",
    "            buggy_lines=sorted(buggy_lines),\n",
    "            fix_nodes=fix_nodes,\n",
    "            edit_operations=edit_operations,\n",
    "            bug_type=bug_type\n",
    "        )\n",
    "    \n",
    "    def extract_contrastive(self, buggy_code: str, fixed_code: str,\n",
    "                           diff_result: DiffResult) -> ContrastiveFeatures:\n",
    "        \"\"\"\n",
    "        Extract features for contrastive learning.\n",
    "        \n",
    "        Creates representations that can be used to:\n",
    "        - Learn to distinguish buggy from fixed code\n",
    "        - Learn edit patterns\n",
    "        - Learn code repair transformations\n",
    "        \"\"\"\n",
    "        # Get AST sequences\n",
    "        try:\n",
    "            buggy_ast = self.ast_parser.parse(buggy_code)\n",
    "            fixed_ast = self.ast_parser.parse(fixed_code)\n",
    "            \n",
    "            buggy_ast_seq = [n.node_type for n in buggy_ast.nodes]\n",
    "            fixed_ast_seq = [n.node_type for n in fixed_ast.nodes]\n",
    "        except Exception:\n",
    "            buggy_ast_seq = []\n",
    "            fixed_ast_seq = []\n",
    "        \n",
    "        # Get token sequences\n",
    "        buggy_tokens = self._tokenize(buggy_code)\n",
    "        fixed_tokens = self._tokenize(fixed_code)\n",
    "        \n",
    "        # Find common and different nodes\n",
    "        buggy_set = set(buggy_ast_seq)\n",
    "        fixed_set = set(fixed_ast_seq)\n",
    "        \n",
    "        common_nodes = list(buggy_set & fixed_set)\n",
    "        diff_nodes = list(buggy_set ^ fixed_set)\n",
    "        \n",
    "        # Get edit path\n",
    "        edit_path = [action.to_string() for action in diff_result.actions]\n",
    "        \n",
    "        return ContrastiveFeatures(\n",
    "            buggy_ast_seq=buggy_ast_seq,\n",
    "            fixed_ast_seq=fixed_ast_seq,\n",
    "            buggy_tokens=buggy_tokens,\n",
    "            fixed_tokens=fixed_tokens,\n",
    "            common_nodes=common_nodes,\n",
    "            diff_nodes=diff_nodes,\n",
    "            edit_path=edit_path\n",
    "        )\n",
    "    \n",
    "    def extract(self, buggy_code: str, fixed_code: str) -> DiffFeatures:\n",
    "        \"\"\"\n",
    "        Extract all features from buggy and fixed code pair.\n",
    "        \n",
    "        Args:\n",
    "            buggy_code: The buggy version of the code\n",
    "            fixed_code: The fixed version of the code\n",
    "            \n",
    "        Returns:\n",
    "            DiffFeatures containing all extracted features\n",
    "        \"\"\"\n",
    "        # Compute AST diff\n",
    "        diff_result = self.gumtree.diff(buggy_code, fixed_code)\n",
    "        \n",
    "        # Extract features\n",
    "        mask = self.extract_mask(buggy_code, diff_result)\n",
    "        localization = self.extract_localization(buggy_code, fixed_code, diff_result)\n",
    "        contrastive = self.extract_contrastive(buggy_code, fixed_code, diff_result)\n",
    "        \n",
    "        # Compute statistics\n",
    "        edit_types = defaultdict(int)\n",
    "        for action in diff_result.actions:\n",
    "            edit_types[action.action_type.value] += 1\n",
    "        \n",
    "        return DiffFeatures(\n",
    "            mask=mask,\n",
    "            localization=localization,\n",
    "            contrastive=contrastive,\n",
    "            diff_result=diff_result,\n",
    "            num_edits=len(diff_result.actions),\n",
    "            edit_types=dict(edit_types)\n",
    "        )\n",
    "    \n",
    "    def extract_for_training(self, buggy_code: str, fixed_code: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract features in format ready for model training.\n",
    "        \n",
    "        Returns dict with:\n",
    "        - input_tokens: Tokenized buggy code\n",
    "        - labels: Binary mask (1 = buggy token)\n",
    "        - ast_sequence: AST node sequence\n",
    "        - edit_sequence: Sequence of edit operations\n",
    "        \"\"\"\n",
    "        features = self.extract(buggy_code, fixed_code)\n",
    "        \n",
    "        return {\n",
    "            \"input_tokens\": features.contrastive.buggy_tokens,\n",
    "            \"labels\": features.mask.token_mask,\n",
    "            \"ast_sequence\": features.contrastive.buggy_ast_seq,\n",
    "            \"edit_sequence\": features.contrastive.edit_path,\n",
    "            \"buggy_lines\": features.localization.buggy_lines,\n",
    "            \"bug_type\": features.localization.bug_type,\n",
    "            \"num_edits\": features.num_edits\n",
    "        }\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def extract_supervision_signals(buggy_code: str, fixed_code: str, \n",
    "                                gumtree_path: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Quick function to extract all supervision signals.\n",
    "    \n",
    "    Returns dict with mask, localization, and contrastive features.\n",
    "    \"\"\"\n",
    "    extractor = DiffFeatureExtractor(gumtree_path=gumtree_path)\n",
    "    features = extractor.extract(buggy_code, fixed_code)\n",
    "    return features.to_dict()\n",
    "\n",
    "\n",
    "def get_buggy_mask(buggy_code: str, fixed_code: str,\n",
    "                   gumtree_path: str = None) -> List[int]:\n",
    "    \"\"\"\n",
    "    Quick function to get token-level buggy mask.\n",
    "    \n",
    "    Returns list of 0/1 indicating buggy tokens.\n",
    "    \"\"\"\n",
    "    extractor = DiffFeatureExtractor(gumtree_path=gumtree_path)\n",
    "    features = extractor.extract(buggy_code, fixed_code)\n",
    "    return features.mask.token_mask\n",
    "\n",
    "\n",
    "def get_buggy_lines(buggy_code: str, fixed_code: str,\n",
    "                    gumtree_path: str = None) -> List[int]:\n",
    "    \"\"\"\n",
    "    Quick function to get buggy line numbers.\n",
    "    \n",
    "    Returns list of line numbers containing bugs.\n",
    "    \"\"\"\n",
    "    extractor = DiffFeatureExtractor(gumtree_path=gumtree_path)\n",
    "    features = extractor.extract(buggy_code, fixed_code)\n",
    "    return features.localization.buggy_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:38:44.557514Z",
     "iopub.status.busy": "2026-01-15T08:38:44.557313Z",
     "iopub.status.idle": "2026-01-15T08:38:55.560009Z",
     "shell.execute_reply": "2026-01-15T08:38:55.559445Z",
     "shell.execute_reply.started": "2026-01-15T08:38:44.557497Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 08:38:50.376474: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768466330.397398   11432 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768466330.404114   11432 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bug Detector module implementing neural network models for bug detection.\n",
    "\n",
    "Provides:\n",
    "1. Dataset class for loading and preprocessing buggy/fixed code pairs\n",
    "2. Bug detection models using diff-based supervision\n",
    "3. Training utilities\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Optional imports - will work without these but with reduced functionality\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer, \n",
    "        AutoModel,\n",
    "        AutoConfig,\n",
    "        PreTrainedModel,\n",
    "        PreTrainedTokenizer\n",
    "    )\n",
    "    HAS_TRANSFORMERS = True\n",
    "except ImportError:\n",
    "    HAS_TRANSFORMERS = False\n",
    "    print(\"Warning: transformers not installed. Some features will be limited.\")\n",
    "\n",
    "# from .feature_extractor import DiffFeatureExtractor, DiffFeatures\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BugDetectionSample:\n",
    "    \"\"\"\n",
    "    A single sample for bug detection training/inference.\n",
    "    \n",
    "    Attributes:\n",
    "        buggy_code: The buggy source code\n",
    "        fixed_code: The fixed source code (for training)\n",
    "        token_labels: Binary labels for each token (1 = buggy)\n",
    "        line_labels: Binary labels for each line\n",
    "        bug_type: Type of bug\n",
    "        features: Extracted diff features\n",
    "    \"\"\"\n",
    "    buggy_code: str\n",
    "    fixed_code: Optional[str] = None\n",
    "    token_labels: List[int] = field(default_factory=list)\n",
    "    line_labels: List[int] = field(default_factory=list)\n",
    "    bug_type: str = \"unknown\"\n",
    "    features: Optional[DiffFeatures] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"buggy_code\": self.buggy_code,\n",
    "            \"fixed_code\": self.fixed_code,\n",
    "            \"token_labels\": self.token_labels,\n",
    "            \"line_labels\": self.line_labels,\n",
    "            \"bug_type\": self.bug_type\n",
    "        }\n",
    "\n",
    "\n",
    "class BugDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for bug detection.\n",
    "    \n",
    "    Loads buggy/fixed code pairs and extracts features using AST diff.\n",
    "    Supports multiple formats: parquet, json, csv.\n",
    "    \n",
    "    Usage:\n",
    "        dataset = BugDetectionDataset(\n",
    "            data_path=\"data/buggy_fixed_pairs.parquet\",\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=512\n",
    "        )\n",
    "        loader = DataLoader(dataset, batch_size=32)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: Optional[str] = None,\n",
    "        data: Optional[List[Dict]] = None,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        max_length: int = 512,\n",
    "        gumtree_path: Optional[str] = None,\n",
    "        extract_features: bool = True,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        buggy_col: str = \"buggy_function\",\n",
    "        fixed_col: str = \"fixed_function\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to data file (parquet, json, or csv)\n",
    "            data: List of dicts with buggy/fixed code (alternative to data_path)\n",
    "            tokenizer: Tokenizer for encoding text\n",
    "            max_length: Maximum sequence length\n",
    "            gumtree_path: Path to GumTree executable\n",
    "            extract_features: Whether to pre-extract AST diff features\n",
    "            cache_dir: Directory to cache extracted features\n",
    "            buggy_col: Column name for buggy code\n",
    "            fixed_col: Column name for fixed code\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.gumtree_path = gumtree_path\n",
    "        self.extract_features = extract_features\n",
    "        self.cache_dir = cache_dir\n",
    "        self.buggy_col = buggy_col\n",
    "        self.fixed_col = fixed_col\n",
    "        \n",
    "        # Load data\n",
    "        if data is not None:\n",
    "            self.samples = data\n",
    "        elif data_path is not None:\n",
    "            self.samples = self._load_data(data_path)\n",
    "        else:\n",
    "            self.samples = []\n",
    "        \n",
    "        # Initialize feature extractor\n",
    "        if extract_features:\n",
    "            self.feature_extractor = DiffFeatureExtractor(gumtree_path=gumtree_path)\n",
    "        else:\n",
    "            self.feature_extractor = None\n",
    "        \n",
    "        # Setup cache\n",
    "        if cache_dir:\n",
    "            os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Pre-extracted features\n",
    "        self._features_cache: Dict[int, DiffFeatures] = {}\n",
    "    \n",
    "    def _load_data(self, data_path: str) -> List[Dict]:\n",
    "        \"\"\"Load data from file\"\"\"\n",
    "        path = Path(data_path)\n",
    "        \n",
    "        if path.suffix == '.parquet':\n",
    "            import pandas as pd\n",
    "            df = pd.read_parquet(data_path)\n",
    "            return df.to_dict('records')\n",
    "        \n",
    "        elif path.suffix == '.json':\n",
    "            with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        \n",
    "        elif path.suffix == '.csv':\n",
    "            import pandas as pd\n",
    "            df = pd.read_csv(data_path)\n",
    "            return df.to_dict('records')\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {path.suffix}\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def _get_cached_features(self, idx: int) -> Optional[DiffFeatures]:\n",
    "        \"\"\"Get cached features if available\"\"\"\n",
    "        if idx in self._features_cache:\n",
    "            return self._features_cache[idx]\n",
    "        \n",
    "        if self.cache_dir:\n",
    "            cache_file = Path(self.cache_dir) / f\"features_{idx}.json\"\n",
    "            if cache_file.exists():\n",
    "                # Load from cache (simplified - would need full deserialization)\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _extract_and_cache_features(self, idx: int, buggy: str, fixed: str) -> DiffFeatures:\n",
    "        \"\"\"Extract features and cache them\"\"\"\n",
    "        features = self.feature_extractor.extract(buggy, fixed)\n",
    "        self._features_cache[idx] = features\n",
    "        \n",
    "        # Save to disk cache\n",
    "        if self.cache_dir:\n",
    "            cache_file = Path(self.cache_dir) / f\"features_{idx}.json\"\n",
    "            with open(cache_file, 'w') as f:\n",
    "                json.dump(features.to_dict(), f)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"Get a single sample\"\"\"\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        buggy_code = sample.get(self.buggy_col, sample.get('buggy_code', ''))\n",
    "        fixed_code = sample.get(self.fixed_col, sample.get('fixed_code', ''))\n",
    "        \n",
    "        # Extract features\n",
    "        if self.extract_features and self.feature_extractor:\n",
    "            features = self._get_cached_features(idx)\n",
    "            if features is None:\n",
    "                try:\n",
    "                    features = self._extract_and_cache_features(idx, buggy_code, fixed_code)\n",
    "                except Exception:\n",
    "                    features = None\n",
    "            \n",
    "            if features is not None:\n",
    "                token_labels = features.mask.token_mask\n",
    "            else:\n",
    "                token_labels = []\n",
    "        else:\n",
    "            token_labels = []\n",
    "        \n",
    "        # Only return tensor-compatible data for batching\n",
    "        if self.tokenizer:\n",
    "            encoding = self.tokenizer(\n",
    "                buggy_code,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Align labels with tokenizer output\n",
    "            labels = self._align_labels_to_tokens(\n",
    "                buggy_code, token_labels, encoding\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"idx\": torch.tensor(idx, dtype=torch.long),\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": labels,\n",
    "            }\n",
    "        else:\n",
    "            # Without tokenizer, return raw data (can't be batched easily)\n",
    "            return {\n",
    "                \"idx\": idx,\n",
    "                \"buggy_code\": buggy_code,\n",
    "                \"fixed_code\": fixed_code,\n",
    "                \"token_labels\": token_labels,\n",
    "            }\n",
    "    \n",
    "    def _align_labels_to_tokens(\n",
    "        self,\n",
    "        code: str,\n",
    "        token_labels: List[int],\n",
    "        encoding: Dict\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Align simple token labels to subword tokenizer output\"\"\"\n",
    "        # This is a simplified version - production code would need\n",
    "        # more sophisticated alignment based on character offsets\n",
    "        \n",
    "        labels = torch.zeros(self.max_length, dtype=torch.long)\n",
    "        \n",
    "        if not token_labels:\n",
    "            return labels\n",
    "        \n",
    "        # For now, spread labels across subwords\n",
    "        # In practice, you'd want to use word_ids() for proper alignment\n",
    "        n_labels = min(len(token_labels), self.max_length)\n",
    "        for i in range(n_labels):\n",
    "            labels[i] = token_labels[i]\n",
    "        \n",
    "        return labels\n",
    "\n",
    "\n",
    "class BugTokenClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Token classification head for bug detection.\n",
    "    \n",
    "    Classifies each token as buggy (1) or not buggy (0).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, 2)  # Binary classification\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, hidden_size]\n",
    "            \n",
    "        Returns:\n",
    "            logits: [batch_size, seq_len, 2]\n",
    "        \"\"\"\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        logits = self.classifier(hidden_states)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class BugLocalizationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence labeling head for bug localization.\n",
    "    \n",
    "    Uses CRF or simple classification to predict buggy spans.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, num_labels: int = 3, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size: Size of input hidden states\n",
    "            num_labels: Number of labels (e.g., 3 for BIO tagging)\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, hidden_size]\n",
    "            \n",
    "        Returns:\n",
    "            logits: [batch_size, seq_len, num_labels]\n",
    "        \"\"\"\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        logits = self.classifier(hidden_states)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ContrastiveHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive learning head for distinguishing buggy vs fixed code.\n",
    "    \n",
    "    Projects representations to embedding space for contrastive loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, projection_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, projection_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: [batch_size, hidden_size] (pooled)\n",
    "            \n",
    "        Returns:\n",
    "            embeddings: [batch_size, projection_dim]\n",
    "        \"\"\"\n",
    "        return F.normalize(self.projector(hidden_states), dim=-1)\n",
    "\n",
    "\n",
    "class BugDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete bug detection model.\n",
    "    \n",
    "    Architecture:\n",
    "    - Encoder: Pre-trained code model (CodeBERT, GraphCodeBERT, etc.)\n",
    "    - Token Classification Head: Identifies buggy tokens\n",
    "    - Localization Head: Predicts buggy spans\n",
    "    - Contrastive Head: Learns to distinguish buggy vs fixed\n",
    "    \n",
    "    Training objectives:\n",
    "    1. Token classification loss (BCE)\n",
    "    2. Span localization loss (CE)\n",
    "    3. Contrastive loss (InfoNCE)\n",
    "    \n",
    "    Usage:\n",
    "        model = BugDetector.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Token-level predictions\n",
    "        buggy_probs = outputs[\"token_probs\"]  # [batch, seq_len]\n",
    "        \n",
    "        # Training\n",
    "        loss = model.compute_loss(outputs, labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: nn.Module,\n",
    "        hidden_size: int = 768,\n",
    "        num_labels: int = 2,\n",
    "        use_contrastive: bool = True,\n",
    "        contrastive_dim: int = 128,\n",
    "        dropout: float = 0.1,\n",
    "        freeze_encoder: bool = False,\n",
    "        freeze_encoder_layers: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize bug detector.\n",
    "        \n",
    "        Args:\n",
    "            encoder: Pre-trained encoder model\n",
    "            hidden_size: Hidden size of encoder\n",
    "            num_labels: Number of classification labels\n",
    "            use_contrastive: Whether to use contrastive learning\n",
    "            contrastive_dim: Dimension of contrastive embeddings\n",
    "            dropout: Dropout probability\n",
    "            freeze_encoder: If True, freeze ALL encoder parameters (only train heads)\n",
    "            freeze_encoder_layers: Number of encoder layers to freeze from bottom (0 = none)\n",
    "                                   e.g., 6 = freeze first 6 layers, train last 6 + heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.use_contrastive = use_contrastive\n",
    "        \n",
    "        # Freeze encoder if requested\n",
    "        if freeze_encoder:\n",
    "            self._freeze_encoder()\n",
    "        elif freeze_encoder_layers > 0:\n",
    "            self._freeze_encoder_layers(freeze_encoder_layers)\n",
    "        \n",
    "        # Classification heads\n",
    "        self.token_head = BugTokenClassificationHead(hidden_size, dropout)\n",
    "        self.localization_head = BugLocalizationHead(hidden_size, num_labels=3, dropout=dropout)\n",
    "        \n",
    "        # Contrastive head\n",
    "        if use_contrastive:\n",
    "            self.contrastive_head = ContrastiveHead(hidden_size, contrastive_dim)\n",
    "        \n",
    "        # Pooler for sequence-level representation\n",
    "        self.pooler = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def _freeze_encoder(self):\n",
    "        \"\"\"Freeze all encoder parameters - only train classification heads\"\"\"\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Encoder frozen: only training classification heads\")\n",
    "    \n",
    "    def _freeze_encoder_layers(self, num_layers: int):\n",
    "        \"\"\"\n",
    "        Freeze first N layers of encoder.\n",
    "        GraphCodeBERT has 12 layers (0-11).\n",
    "        \n",
    "        Args:\n",
    "            num_layers: Number of layers to freeze from bottom\n",
    "        \"\"\"\n",
    "        # Freeze embeddings\n",
    "        if hasattr(self.encoder, 'embeddings'):\n",
    "            for param in self.encoder.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Freeze specified number of transformer layers\n",
    "        if hasattr(self.encoder, 'encoder') and hasattr(self.encoder.encoder, 'layer'):\n",
    "            layers = self.encoder.encoder.layer\n",
    "            for i, layer in enumerate(layers):\n",
    "                if i < num_layers:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "        \n",
    "        print(f\"Frozen: embeddings + first {num_layers} encoder layers\")\n",
    "    \n",
    "    def unfreeze_encoder(self):\n",
    "        \"\"\"Unfreeze all encoder parameters for full fine-tuning\"\"\"\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Encoder unfrozen: full fine-tuning enabled\")\n",
    "    \n",
    "    def get_trainable_params_info(self) -> Dict[str, int]:\n",
    "        \"\"\"Get info about trainable vs frozen parameters\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        frozen_params = total_params - trainable_params\n",
    "        \n",
    "        return {\n",
    "            \"total\": total_params,\n",
    "            \"trainable\": trainable_params,\n",
    "            \"frozen\": frozen_params,\n",
    "            \"trainable_percent\": 100 * trainable_params / total_params\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_name: str = \"microsoft/graphcodebert-base\",\n",
    "        use_contrastive: bool = True,\n",
    "        freeze_encoder: bool = False,\n",
    "        freeze_encoder_layers: int = 0,\n",
    "        **kwargs\n",
    "    ) -> \"BugDetector\":\n",
    "        \"\"\"\n",
    "        Create BugDetector from pre-trained model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of pre-trained model on HuggingFace\n",
    "            use_contrastive: Whether to use contrastive learning\n",
    "            freeze_encoder: If True, freeze ALL encoder (only train heads)\n",
    "            freeze_encoder_layers: Freeze first N layers (0=none, 6=half, 12=all)\n",
    "            \n",
    "        Returns:\n",
    "            Initialized BugDetector\n",
    "        \"\"\"\n",
    "        if not HAS_TRANSFORMERS:\n",
    "            raise ImportError(\"transformers library required for from_pretrained\")\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        encoder = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        return cls(\n",
    "            encoder=encoder,\n",
    "            hidden_size=config.hidden_size,\n",
    "            use_contrastive=use_contrastive,\n",
    "            freeze_encoder=freeze_encoder,\n",
    "            freeze_encoder_layers=freeze_encoder_layers,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len]\n",
    "            token_type_ids: [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Dict with:\n",
    "            - token_logits: [batch_size, seq_len, 2]\n",
    "            - token_probs: [batch_size, seq_len]\n",
    "            - span_logits: [batch_size, seq_len, 3]\n",
    "            - sequence_embedding: [batch_size, hidden_size]\n",
    "            - contrastive_embedding: [batch_size, contrastive_dim]\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state  # [batch, seq, hidden]\n",
    "        \n",
    "        # Token classification\n",
    "        token_logits = self.token_head(hidden_states)\n",
    "        token_probs = F.softmax(token_logits, dim=-1)[:, :, 1]  # Prob of buggy\n",
    "        \n",
    "        # Span localization\n",
    "        span_logits = self.localization_head(hidden_states)\n",
    "        \n",
    "        # Sequence-level representation\n",
    "        cls_hidden = hidden_states[:, 0, :]  # [CLS] token\n",
    "        sequence_embedding = self.pooler(cls_hidden)\n",
    "        \n",
    "        result = {\n",
    "            \"token_logits\": token_logits,\n",
    "            \"token_probs\": token_probs,\n",
    "            \"span_logits\": span_logits,\n",
    "            \"sequence_embedding\": sequence_embedding,\n",
    "            \"hidden_states\": hidden_states,\n",
    "        }\n",
    "        \n",
    "        # Contrastive embedding\n",
    "        if self.use_contrastive:\n",
    "            contrastive_embedding = self.contrastive_head(sequence_embedding)\n",
    "            result[\"contrastive_embedding\"] = contrastive_embedding\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        outputs: Dict[str, torch.Tensor],\n",
    "        token_labels: Optional[torch.Tensor] = None,\n",
    "        span_labels: Optional[torch.Tensor] = None,\n",
    "        contrastive_labels: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        alpha: float = 1.0,\n",
    "        beta: float = 0.5,\n",
    "        gamma: float = 0.1,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute training losses.\n",
    "        \n",
    "        Args:\n",
    "            outputs: Forward pass outputs\n",
    "            token_labels: [batch_size, seq_len] - binary labels\n",
    "            span_labels: [batch_size, seq_len] - BIO labels\n",
    "            contrastive_labels: Labels for contrastive loss\n",
    "            attention_mask: Mask for valid tokens\n",
    "            alpha: Weight for token classification loss\n",
    "            beta: Weight for span localization loss\n",
    "            gamma: Weight for contrastive loss\n",
    "            \n",
    "        Returns:\n",
    "            Dict with individual losses and total loss\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Token classification loss\n",
    "        if token_labels is not None:\n",
    "            token_logits = outputs[\"token_logits\"]\n",
    "            token_loss = F.cross_entropy(\n",
    "                token_logits.view(-1, 2),\n",
    "                token_labels.view(-1),\n",
    "                reduction='none'\n",
    "            )\n",
    "            \n",
    "            # Mask padding\n",
    "            if attention_mask is not None:\n",
    "                token_loss = token_loss * attention_mask.view(-1)\n",
    "                token_loss = token_loss.sum() / attention_mask.sum()\n",
    "            else:\n",
    "                token_loss = token_loss.mean()\n",
    "            \n",
    "            losses[\"token_loss\"] = token_loss\n",
    "            total_loss += alpha * token_loss\n",
    "        \n",
    "        # Span localization loss\n",
    "        if span_labels is not None:\n",
    "            span_logits = outputs[\"span_logits\"]\n",
    "            span_loss = F.cross_entropy(\n",
    "                span_logits.view(-1, 3),\n",
    "                span_labels.view(-1),\n",
    "                reduction='none'\n",
    "            )\n",
    "            \n",
    "            if attention_mask is not None:\n",
    "                span_loss = span_loss * attention_mask.view(-1)\n",
    "                span_loss = span_loss.sum() / attention_mask.sum()\n",
    "            else:\n",
    "                span_loss = span_loss.mean()\n",
    "            \n",
    "            losses[\"span_loss\"] = span_loss\n",
    "            total_loss += beta * span_loss\n",
    "        \n",
    "        # Contrastive loss (InfoNCE)\n",
    "        if self.use_contrastive and \"contrastive_embedding\" in outputs:\n",
    "            if contrastive_labels is not None:\n",
    "                contrastive_emb = outputs[\"contrastive_embedding\"]\n",
    "                contrastive_loss = self._compute_contrastive_loss(\n",
    "                    contrastive_emb, contrastive_labels\n",
    "                )\n",
    "                losses[\"contrastive_loss\"] = contrastive_loss\n",
    "                total_loss += gamma * contrastive_loss\n",
    "        \n",
    "        losses[\"total_loss\"] = total_loss\n",
    "        return losses\n",
    "    \n",
    "    def _compute_contrastive_loss(\n",
    "        self,\n",
    "        embeddings: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        temperature: float = 0.07\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute InfoNCE contrastive loss.\n",
    "        \n",
    "        Pairs with same label should be closer than pairs with different labels.\n",
    "        \"\"\"\n",
    "        # Similarity matrix\n",
    "        similarity = torch.matmul(embeddings, embeddings.T) / temperature\n",
    "        \n",
    "        # Create mask for positive pairs (same label)\n",
    "        labels = labels.view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float()\n",
    "        \n",
    "        # Exclude diagonal\n",
    "        logits_mask = torch.ones_like(mask) - torch.eye(mask.size(0), device=mask.device)\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        # Compute loss\n",
    "        exp_sim = torch.exp(similarity) * logits_mask\n",
    "        log_prob = similarity - torch.log(exp_sim.sum(dim=1, keepdim=True))\n",
    "        \n",
    "        # Mean of positive pairs\n",
    "        mean_log_prob = (mask * log_prob).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "        loss = -mean_log_prob.mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        threshold: float = 0.5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Make predictions on input.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            attention_mask: Attention mask\n",
    "            threshold: Threshold for binary classification\n",
    "            \n",
    "        Returns:\n",
    "            Dict with predictions and probabilities\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(input_ids, attention_mask)\n",
    "        \n",
    "        token_probs = outputs[\"token_probs\"]\n",
    "        predictions = (token_probs > threshold).long()\n",
    "        \n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"probabilities\": token_probs,\n",
    "            \"buggy_positions\": [\n",
    "                (p > threshold).nonzero().squeeze(-1).tolist()\n",
    "                for p in token_probs\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class BugDetectorTrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for BugDetector model.\n",
    "    \n",
    "    Handles training loop, evaluation, and checkpointing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: BugDetector,\n",
    "        train_dataset: BugDetectionDataset,\n",
    "        eval_dataset: Optional[BugDetectionDataset] = None,\n",
    "        optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "        lr: float = 2e-5,\n",
    "        batch_size: int = 16,\n",
    "        num_epochs: int = 3,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        output_dir: str = \"./output\",\n",
    "        show_progress: bool = True,\n",
    "        log_steps: int = 10,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize trainer.\n",
    "        \n",
    "        Args:\n",
    "            model: BugDetector model\n",
    "            train_dataset: Training dataset\n",
    "            eval_dataset: Evaluation dataset\n",
    "            optimizer: Optimizer (default: AdamW)\n",
    "            lr: Learning rate\n",
    "            batch_size: Batch size\n",
    "            num_epochs: Number of training epochs\n",
    "            device: Device to train on\n",
    "            output_dir: Directory for checkpoints\n",
    "            show_progress: If True, show tqdm progress bar\n",
    "            log_steps: Log training loss every N steps (0 to disable step logging)\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        self.num_epochs = num_epochs\n",
    "        self.show_progress = show_progress\n",
    "        self.log_steps = log_steps\n",
    "        \n",
    "        # Data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        if eval_dataset:\n",
    "            self.eval_loader = DataLoader(\n",
    "                eval_dataset, \n",
    "                batch_size=batch_size\n",
    "            )\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optimizer or torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=lr\n",
    "        )\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def train(self) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Run training loop.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with training history (train_loss, test_loss, train_acc, test_acc, train_f1, test_f1)\n",
    "        \"\"\"\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        history = {\n",
    "            \"train_loss\": [],\n",
    "            \"test_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"test_acc\": [],\n",
    "            \"train_f1\": [],\n",
    "            \"test_f1\": []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            epoch_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_tp, train_fp, train_fn = 0, 0, 0\n",
    "            \n",
    "            # Wrap with tqdm if show_progress is enabled\n",
    "            train_iter = self.train_loader\n",
    "            if self.show_progress:\n",
    "                train_iter = tqdm(\n",
    "                    self.train_loader, \n",
    "                    desc=f\"Epoch {epoch+1}/{self.num_epochs}\",\n",
    "                    leave=True\n",
    "                )\n",
    "            \n",
    "            for batch in train_iter:\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                losses = self.model.compute_loss(\n",
    "                    outputs, \n",
    "                    token_labels=labels,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                loss = losses[\"total_loss\"]\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                # Compute training metrics\n",
    "                with torch.no_grad():\n",
    "                    preds = (outputs[\"token_probs\"] > 0.5).long()\n",
    "                    mask = attention_mask.bool()\n",
    "                    train_correct += ((preds == labels) & mask).sum().item()\n",
    "                    train_total += mask.sum().item()\n",
    "                    \n",
    "                    # F1 components\n",
    "                    train_tp += ((preds == 1) & (labels == 1) & mask).sum().item()\n",
    "                    train_fp += ((preds == 1) & (labels == 0) & mask).sum().item()\n",
    "                    train_fn += ((preds == 0) & (labels == 1) & mask).sum().item()\n",
    "                \n",
    "                # Update progress bar with current loss\n",
    "                if self.show_progress:\n",
    "                    train_iter.set_postfix(loss=loss.item())\n",
    "            \n",
    "            # Compute epoch metrics\n",
    "            train_loss = epoch_loss / len(self.train_loader)\n",
    "            train_acc = train_correct / train_total if train_total > 0 else 0.0\n",
    "            train_precision = train_tp / (train_tp + train_fp) if (train_tp + train_fp) > 0 else 0.0\n",
    "            train_recall = train_tp / (train_tp + train_fn) if (train_tp + train_fn) > 0 else 0.0\n",
    "            train_f1 = 2 * train_precision * train_recall / (train_precision + train_recall) if (train_precision + train_recall) > 0 else 0.0\n",
    "            \n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"train_acc\"].append(train_acc)\n",
    "            history[\"train_f1\"].append(train_f1)\n",
    "            \n",
    "            # Evaluation\n",
    "            if self.eval_dataset:\n",
    "                eval_metrics = self.evaluate()\n",
    "                history[\"test_loss\"].append(eval_metrics[\"loss\"])\n",
    "                history[\"test_acc\"].append(eval_metrics[\"accuracy\"])\n",
    "                history[\"test_f1\"].append(eval_metrics[\"f1\"])\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{self.num_epochs} - \"\n",
    "                      f\"Train Loss: {train_loss:.4f} - Test Loss: {eval_metrics['loss']:.4f} - \"\n",
    "                      f\"Train Acc: {train_acc:.4f} - Test Acc: {eval_metrics['accuracy']:.4f} - \"\n",
    "                      f\"Train F1: {train_f1:.4f} - Test F1: {eval_metrics['f1']:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{self.num_epochs} - \"\n",
    "                      f\"Train Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f} - Train F1: {train_f1:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self.save_checkpoint(epoch)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def evaluate(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate model on eval dataset.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with evaluation metrics (loss, accuracy, f1)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.eval_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                losses = self.model.compute_loss(\n",
    "                    outputs,\n",
    "                    token_labels=labels,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                total_loss += losses[\"total_loss\"].item()\n",
    "                \n",
    "                # Compute accuracy\n",
    "                preds = (outputs[\"token_probs\"] > 0.5).long()\n",
    "                mask = attention_mask.bool()\n",
    "                correct += ((preds == labels) & mask).sum().item()\n",
    "                total += mask.sum().item()\n",
    "                \n",
    "                # F1 components\n",
    "                tp += ((preds == 1) & (labels == 1) & mask).sum().item()\n",
    "                fp += ((preds == 1) & (labels == 0) & mask).sum().item()\n",
    "                fn += ((preds == 0) & (labels == 1) & mask).sum().item()\n",
    "        \n",
    "        # Compute F1\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            \"loss\": total_loss / len(self.eval_loader),\n",
    "            \"accuracy\": correct / total if total > 0 else 0.0,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "    \n",
    "    def save_checkpoint(self, epoch: int):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        path = os.path.join(self.output_dir, f\"checkpoint-{epoch}.pt\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "        }, path)\n",
    "    \n",
    "    def load_checkpoint(self, path: str):\n",
    "        \"\"\"Load model checkpoint\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        return checkpoint[\"epoch\"]\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def create_bug_detector(\n",
    "    model_name: str = \"microsoft/graphcodebert-base\",\n",
    "    device: str = None\n",
    ") -> Tuple[BugDetector, Any]:\n",
    "    \"\"\"\n",
    "    Create bug detector with tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Pre-trained model name\n",
    "        device: Device to use\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model, tokenizer)\n",
    "    \"\"\"\n",
    "    if not HAS_TRANSFORMERS:\n",
    "        raise ImportError(\"transformers library required\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = BugDetector.from_pretrained(model_name)\n",
    "    \n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def train_bug_detector(\n",
    "    train_data_path: str,\n",
    "    model_name: str = \"microsoft/graphcodebert-base\",\n",
    "    output_dir: str = \"./output\",\n",
    "    num_epochs: int = 3,\n",
    "    batch_size: int = 16,\n",
    "    lr: float = 2e-5,\n",
    "    gumtree_path: str = None,\n",
    ") -> BugDetector:\n",
    "    \"\"\"\n",
    "    Train a bug detector model.\n",
    "    \n",
    "    Args:\n",
    "        train_data_path: Path to training data\n",
    "        model_name: Pre-trained model name\n",
    "        output_dir: Output directory\n",
    "        num_epochs: Number of epochs\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        gumtree_path: Path to GumTree\n",
    "        \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    model, tokenizer = create_bug_detector(model_name)\n",
    "    \n",
    "    dataset = BugDetectionDataset(\n",
    "        data_path=train_data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        gumtree_path=gumtree_path,\n",
    "        extract_features=True\n",
    "    )\n",
    "    \n",
    "    trainer = BugDetectorTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        output_dir=output_dir,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:38:55.562141Z",
     "iopub.status.busy": "2026-01-15T08:38:55.561350Z",
     "iopub.status.idle": "2026-01-15T08:39:00.730210Z",
     "shell.execute_reply": "2026-01-15T08:39:00.729385Z",
     "shell.execute_reply.started": "2026-01-15T08:38:55.562120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_single = pd.read_parquet('megadiff_single_function.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:39:00.731551Z",
     "iopub.status.busy": "2026-01-15T08:39:00.731246Z",
     "iopub.status.idle": "2026-01-15T08:39:02.186912Z",
     "shell.execute_reply": "2026-01-15T08:39:02.186260Z",
     "shell.execute_reply.started": "2026-01-15T08:39:00.731525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8000\n",
      "\n",
      "=== Single sample check ===\n",
      "  idx: shape=torch.Size([]), dtype=torch.int64\n",
      "  input_ids: shape=torch.Size([512]), dtype=torch.int64\n",
      "  attention_mask: shape=torch.Size([512]), dtype=torch.int64\n",
      "  labels: shape=torch.Size([512]), dtype=torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Training Pipeline Example (vi subset nh  demo nhanh)\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "\n",
    "# 2. Create Dataset vi subset nh  demo (first 100 samples)\n",
    "samples = df_single.head(10000).to_dict('records')\n",
    "\n",
    "\n",
    "\n",
    "train_samples, val_samples = train_test_split(\n",
    "    samples, \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = BugDetectionDataset(\n",
    "    data=train_samples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    gumtree_path=\"gumtree-4.0.0-beta4/bin/gumtree\",\n",
    "    extract_features=True,\n",
    "    buggy_col=\"buggy_function\",\n",
    "    fixed_col=\"fixed_function\"\n",
    ")\n",
    "\n",
    "eval_dataset = BugDetectionDataset(\n",
    "    data=val_samples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    gumtree_path=\"gumtree-4.0.0-beta4/bin/gumtree\",\n",
    "    extract_features=True,\n",
    "    buggy_col=\"buggy_function\",\n",
    "    fixed_col=\"fixed_function\"\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "# Test single sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\n=== Single sample check ===\")\n",
    "for k, v in sample.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"  {k}: shape={v.shape}, dtype={v.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {type(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:39:02.187789Z",
     "iopub.status.busy": "2026-01-15T08:39:02.187608Z",
     "iopub.status.idle": "2026-01-15T08:39:05.491877Z",
     "shell.execute_reply": "2026-01-15T08:39:05.491075Z",
     "shell.execute_reply.started": "2026-01-15T08:39:02.187773Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen: embeddings + first 6 encoder layers\n",
      "\n",
      "=== Model Parameters (partial) ===\n",
      "Total: 125,929,093\n",
      "Trainable: 44,401,285 (35.3%)\n",
      "Frozen: 81,527,808\n"
     ]
    }
   ],
   "source": [
    "# 4. Create Model vi cc ty chn freeze\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============ CHN CHIN LC FINE-TUNING ============\n",
    "# Option 1: Full fine-tuning (train ALL parameters) - Tt nht nu c  data v GPU\n",
    "# Option 2: Freeze encoder (ch train heads) - Nhanh, t overfitting, cn t data\n",
    "# Option 3: Freeze N layers u - Cn bng gia 2 options trn\n",
    "\n",
    "FREEZE_STRATEGY = \"partial\"  # \"full\", \"heads_only\", \"partial\"\n",
    "\n",
    "if FREEZE_STRATEGY == \"full\":\n",
    "    # Full fine-tuning: Train ton b ~125M params\n",
    "    model = BugDetector.from_pretrained(\n",
    "        \"microsoft/graphcodebert-base\",\n",
    "        freeze_encoder=False,\n",
    "        freeze_encoder_layers=0\n",
    "    )\n",
    "elif FREEZE_STRATEGY == \"heads_only\":\n",
    "    # Ch train heads: ~2M params, nhanh nht\n",
    "    model = BugDetector.from_pretrained(\n",
    "        \"microsoft/graphcodebert-base\",\n",
    "        freeze_encoder=True\n",
    "    )\n",
    "else:  # \"partial\"\n",
    "    # Freeze 6 layers u, train 6 layers cui + heads: ~65M params\n",
    "    model = BugDetector.from_pretrained(\n",
    "        \"microsoft/graphcodebert-base\",\n",
    "        freeze_encoder_layers=6  # GraphCodeBERT c 12 layers\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Show trainable params info\n",
    "params_info = model.get_trainable_params_info()\n",
    "print(f\"\\n=== Model Parameters ({FREEZE_STRATEGY}) ===\")\n",
    "print(f\"Total: {params_info['total']:,}\")\n",
    "print(f\"Trainable: {params_info['trainable']:,} ({params_info['trainable_percent']:.1f}%)\")\n",
    "print(f\"Frozen: {params_info['frozen']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T08:39:05.493090Z",
     "iopub.status.busy": "2026-01-15T08:39:05.492739Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   3%|         | 4/125 [03:14<1:37:29, 48.35s/it, loss=0.415]"
     ]
    }
   ],
   "source": [
    "trainer = BugDetectorTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    num_epochs=3,\n",
    "    batch_size=64,\n",
    "    lr=2e-5,\n",
    "    device=device,\n",
    "    output_dir=\"./output\"\n",
    ")\n",
    "history = trainer.train()\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Final loss: {history['train_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get sample buggy/fixed code pair\n",
    "sample = df_single.iloc[0]\n",
    "buggy_code = sample['buggy_function']\n",
    "fixed_code = sample['fixed_function']\n",
    "\n",
    "print(\"=== Buggy Code ===\")\n",
    "print(buggy_code[-100:])\n",
    "print(\"\\n=== Fixed Code ===\")\n",
    "print(fixed_code[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 7. Inference demo\n",
    "model.eval()\n",
    "\n",
    "test_encoding = tokenizer(\n",
    "    buggy_code,\n",
    "    max_length=512,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model.predict(\n",
    "        input_ids=test_encoding['input_ids'].to(device),\n",
    "        attention_mask=test_encoding['attention_mask'].to(device),\n",
    "        threshold=0.5\n",
    "    )\n",
    "\n",
    "print(\"=== Inference Results ===\")\n",
    "print(f\"Predictions shape: {predictions['predictions'].shape}\")\n",
    "print(f\"Number of buggy positions detected: {len(predictions['buggy_positions'][0])}\")\n",
    "\n",
    "# Show first few buggy tokens\n",
    "if predictions['buggy_positions'][0]:\n",
    "    tokens = tokenizer.convert_ids_to_tokens(test_encoding['input_ids'][0])\n",
    "    print(f\"\\nSample buggy tokens (first 10):\")\n",
    "    for pos in predictions['buggy_positions'][0][:10]:\n",
    "        print(f\"  Position {pos}: {tokens[pos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "gumtree_path = \"gumtree-4.0.0-beta4/bin/gumtree\"\n",
    "cache_path = Path(\"precomputed_labels.parquet\")\n",
    "\n",
    "extractor = DiffFeatureExtractor(gumtree_path=gumtree_path)\n",
    "\n",
    "def precompute_labels(df, cache_path, max_rows=None):\n",
    "    rows = df if max_rows is None else df.head(max_rows)\n",
    "    cached = []\n",
    "    for _, row in tqdm(rows.iterrows(), total=len(rows)):\n",
    "        buggy = row[\"buggy_function\"]\n",
    "        fixed = row[\"fixed_function\"]\n",
    "        feats = extractor.extract(buggy, fixed)\n",
    "        cached.append({\n",
    "            \"buggy_function\": buggy,\n",
    "            \"fixed_function\": fixed,\n",
    "            \"token_labels\": feats.mask.token_mask,\n",
    "            \"bug_type\": feats.localization.bug_type,\n",
    "        })\n",
    "    import pandas as pd\n",
    "    pd.DataFrame(cached).to_parquet(cache_path, index=False)\n",
    "\n",
    "class CachedBugDataset(BugDetectionDataset):\n",
    "    def __init__(self, data_path, tokenizer, max_length=512):\n",
    "        import pandas as pd\n",
    "        data = pd.read_parquet(data_path).to_dict(\"records\")\n",
    "        super().__init__(\n",
    "            data=data,\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=max_length,\n",
    "            extract_features=False,   # tt diff khi __getitem__\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.samples[idx]\n",
    "        buggy_code = sample[\"buggy_function\"]\n",
    "        token_labels = sample.get(\"token_labels\", [])\n",
    "        encoding = self.tokenizer(\n",
    "            buggy_code,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = torch.zeros(self.max_length, dtype=torch.long)\n",
    "        n = min(len(token_labels), self.max_length)\n",
    "        if n:\n",
    "            labels[:n] = torch.tensor(token_labels[:n], dtype=torch.long)\n",
    "        return {\n",
    "            \"idx\": torch.tensor(idx, dtype=torch.long),\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "precompute_labels(df_single, cache_path, max_rows=500)  # b max_rows  x l ton b\n",
    "\n",
    "# Dng dataset  cache\n",
    "train_dataset = CachedBugDataset(\"precomputed_labels.parquet\", tokenizer, max_length=512)\n",
    "eval_dataset = train_dataset  # hoc mt file cache khc cho val"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31240,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
